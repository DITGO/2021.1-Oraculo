{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date: 2021/01\n",
    "\n",
    "#### SUMMARY:\n",
    "\n",
    "- This notebook represents the project quality analysis of the date exposed right above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEAM:\n",
    "\n",
    "##### Semester: 2021/01\n",
    "##### Professor: Hilmer Neri\n",
    "\n",
    "##### Members:\n",
    "\n",
    "- Érico Maximiano Bandeira\n",
    "- Henrique Martins de Messias\n",
    "- João Vitor Moura Rosa\n",
    "- Max Henrique Barbosa\n",
    "- Victor Rodrigues Silva\n",
    "- Antonio Igor Carvalho\n",
    "- Gabriel Santos Silva Araújo\n",
    "- João Paulo Lima da Silva\n",
    "- Lucas Vieira de Jesus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# Deal with API request\n",
    "import urllib3\n",
    "from urllib3 import request\n",
    "\n",
    "# Deal with visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAPH SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style('darkgrid',\n",
    "              {'xtick.bottom' : True,\n",
    "               'ytick.left': True,\n",
    "               'grid.linestyle':'--',\n",
    "               'font.monospace': ['Computer Modern Typewriter'],\n",
    "               'axes.edgecolor' : 'white'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATAFRAME SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SonarCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Path to the folder with all your jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = glob('../analytics-raw-data/*.json') # add the path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_path):\n",
    "    \n",
    "    with open(json_path) as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        \n",
    "    return json_obj\n",
    "\n",
    "def create_base_component_df(json_list):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for i in json_list:\n",
    "        \n",
    "        if i.endswith(\"-test.json\"):\n",
    "            continue\n",
    "\n",
    "        base_component = read_json(i)\n",
    "        \n",
    "        base_component_data = base_component['baseComponent']['measures']\n",
    "\n",
    "        base_component_df = pd.DataFrame(base_component_data)\n",
    "\n",
    "        base_component_df['filename'] = os.path.basename(i)\n",
    "\n",
    "        df = df.append(base_component_df, ignore_index=True)\n",
    "\n",
    "    aux_df = df['filename'].str.split(r\"Oraculo-(.*?)-*-(.*?).json\", expand=True)\n",
    "\n",
    "    df['repository'] = aux_df[1]\n",
    "\n",
    "    df['version'] = aux_df[2]\n",
    "\n",
    "    df = df.sort_values(by=['repository', 'version'])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create base component dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_component_df = create_base_component_df(jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_component_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframe per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_list = ['files',\n",
    "               'functions',\n",
    "               'complexity',\n",
    "               'comment_lines_density',\n",
    "               'duplicated_lines_density',\n",
    "               'coverage',\n",
    "               'ncloc',\n",
    "               'security_rating',\n",
    "               'tests',\n",
    "               'test_success_density',\n",
    "               'test_execution_time',\n",
    "               'reliability_rating']\n",
    "\n",
    "len(metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_per_file(json):\n",
    "    \n",
    "    file_json = []\n",
    "    \n",
    "    for component in json['components']:\n",
    "        if component['qualifier'] == 'FIL':\n",
    "            file_json.append(component)\n",
    "            \n",
    "    return file_json\n",
    "\n",
    "def generate_file_dataframe_per_release(metric_list, json, language_extension):\n",
    "    \n",
    "    df_columns = metric_list\n",
    "    df = pd.DataFrame(columns = df_columns)\n",
    "    \n",
    "    for file in json:\n",
    "        try:\n",
    "            if file['language'] == language_extension:\n",
    "                for measure in file['measures']:\n",
    "                    df.at[file['path'], measure['metric']] = measure['value']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    df.reset_index(inplace = True)\n",
    "    df = df.rename({'index': 'path'}, axis=1).drop(['files'], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_file_df(json_list):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for i in json_list:\n",
    "        \n",
    "        if i.endswith(\"-test.json\"):\n",
    "            continue\n",
    "\n",
    "        file_component = read_json(i)\n",
    "        \n",
    "        file_component_data = metric_per_file(file_component)\n",
    "\n",
    "        file_component_df = generate_file_dataframe_per_release(metric_list, file_component_data, language_extension = 'js')\n",
    "        \n",
    "        test_file = i.replace(\".json\", \"-test.json\")\n",
    "        \n",
    "        test_json = read_json(test_file)\n",
    "\n",
    "        file_component_df['tests'] = 1\n",
    "        file_component_df['test_execution_time'] = test_json['test_execution_time']\n",
    "\n",
    "        file_component_df['filename'] = os.path.basename(i)\n",
    "\n",
    "        df = df.append(file_component_df, ignore_index=True)\n",
    "          \n",
    "    aux_df = df['filename'].str.split(r\"Oraculo-(.*?)-*-(.*?).json\", expand=True)\n",
    "    \n",
    "    df['repository'] = aux_df[1]\n",
    "    \n",
    "    df['version'] = aux_df[2]\n",
    "    \n",
    "    df = df.sort_values(by=['repository', 'version'])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_component_df = create_file_df(jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_component_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframe per repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "frontEnd_df = file_component_df[file_component_df['repository'] == 'FrontEnd']\n",
    "tags_df = file_component_df[file_component_df['repository'] == 'Tags']\n",
    "profile_df = file_component_df[file_component_df['repository'] == 'Profile']\n",
    "processos_df = file_component_df[file_component_df['repository'] == 'Registros']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _ncloc(df):\n",
    "    ncloc = 0\n",
    "    for each in df['ncloc']:\n",
    "        ncloc += int(each)\n",
    "\n",
    "    return ncloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure calculations according Q-Rapids quality model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Aspect - Maintainability\n",
    "## Factor - Code Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### COMPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m1(df):\n",
    "    \n",
    "    density_non_complex_files = len(df[(df['complexity'].astype(float)/df['functions'].astype(float)) < 10])/len(df)\n",
    "    \n",
    "    return density_non_complex_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### COMMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m2(df):\n",
    "    \n",
    "    density_comment_files = len(df[(df['comment_lines_density'].astype(float) > 10) & (df['comment_lines_density'].astype(float) < 30)])/len(df)\n",
    "    \n",
    "    return density_comment_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DUPLICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m3(df):\n",
    "    \n",
    "    duplication = len(df[(df['duplicated_lines_density'].astype(float) < 5)])/len(df)\n",
    "    \n",
    "    return duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Aspect - Reliability\n",
    "## Factor - Testing Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Passed tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def m4(df):\n",
    "\n",
    "    passed_tests = df['test_success_density'].astype(float).median() / 100\n",
    "\n",
    "    return passed_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fast test builds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def m5(df):\n",
    "\n",
    "    density_fast_test_builds = len(df[(df['test_execution_time'].astype(float)) < 300]) / sum(df['tests'].astype(float))\n",
    "\n",
    "    return density_fast_test_builds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def m6(df):\n",
    "\n",
    "    density_test_coverage = len(df[(df['coverage'].astype(float) > 60)]) / len(df)\n",
    "\n",
    "    return density_test_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate m1, m2, m3, m4, m5 and m6 for each repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_df(df):\n",
    "    \n",
    "    version_vec = df['version'].unique()\n",
    "    \n",
    "    m1_list = []\n",
    "    m2_list = []\n",
    "    m3_list = []\n",
    "    m4_list = []\n",
    "    m5_list = []\n",
    "    m6_list = []\n",
    "\n",
    "    ncloc_list = []\n",
    "    repository_list = []\n",
    "    version_list = []\n",
    "    \n",
    "    metrics_df = pd.DataFrame()\n",
    "    \n",
    "    for version in version_vec:\n",
    "\n",
    "        version_df = df[df['version'] == version]\n",
    "\n",
    "        m1_list.append(m1(version_df))\n",
    "        m2_list.append(m2(version_df))\n",
    "        m3_list.append(m3(version_df))\n",
    "        m4_list.append(m4(version_df))\n",
    "        m5_list.append(m5(version_df))\n",
    "        m6_list.append(m6(version_df))\n",
    "\n",
    "        ncloc_list.append(_ncloc(version_df))\n",
    "        repository_list.append(version_df['repository'].iloc[0])\n",
    "        version_list.append(version)\n",
    "        \n",
    "    metrics_df = pd.DataFrame({'m1': m1_list,\n",
    "                               'm2': m2_list,\n",
    "                               'm3': m3_list,\n",
    "                               'm4': m4_list,\n",
    "                               'm5': m5_list,\n",
    "                               'm6': m6_list,\n",
    "                               'repository': repository_list, \n",
    "                               'version': version_list,\n",
    "                               'ncloc': ncloc_list})\n",
    "        \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontEnd_metrics = create_metrics_df(frontEnd_df)\n",
    "tags_metrics = create_metrics_df(tags_df)\n",
    "profile_metrics = create_metrics_df(profile_df)\n",
    "processos_metrics = create_metrics_df(processos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "\n",
    "- You must do this for each of your repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot(frontEnd_metrics['m1'], label='Complexity', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(frontEnd_metrics['m2'], label='Comments', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(frontEnd_metrics['m3'], label='Few duplications', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(frontEnd_metrics['m4'], label='Passed tests', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(frontEnd_metrics['m5'], label='Fast test builds', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(frontEnd_metrics['m6'], label='Test coverage', linewidth=3, marker='o', markersize=10)\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot(tags_metrics['m1'], label='Complexity', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(tags_metrics['m2'], label='Comments', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(tags_metrics['m3'], label='Few duplications', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(tags_metrics['m4'], label='Passed tests', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(tags_metrics['m5'], label='Fast test builds', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(tags_metrics['m6'], label='Test coverage', linewidth=3, marker='o', markersize=10)\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot(profile_metrics['m1'], label='Complexity', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(profile_metrics['m2'], label='Comments', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(profile_metrics['m3'], label='Few duplications', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(profile_metrics['m4'], label='Passed tests', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(profile_metrics['m5'], label='Fast test builds', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(profile_metrics['m6'], label='Test coverage', linewidth=3, marker='o', markersize=10)\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot(processos_metrics['m1'], label='Complexity', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(processos_metrics['m2'], label='Comments', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(processos_metrics['m3'], label='Few duplications', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(processos_metrics['m4'], label='Passed tests', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(processos_metrics['m5'], label='Fast test builds', linewidth=3, marker='o', markersize=10)\n",
    "plt.plot(processos_metrics['m6'], label='Test coverage', linewidth=3, marker='o', markersize=10)\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub characteristic aggregation\n",
    "\n",
    "- You must do this for each of your repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psc1, psc2 = 1, 1\n",
    "pc1, pc2 = 0.5, 0.5\n",
    "pm1 = 0.33\n",
    "pm2 = 0.33\n",
    "pm3 = 0.33\n",
    "pm4 = 0.15\n",
    "pm5 = 0.15\n",
    "pm6 = 0.7\n",
    "\n",
    "frontEnd_metrics['code_quality'] = ((frontEnd_metrics['m1']*pm1) + (frontEnd_metrics['m2']*pm2) + (frontEnd_metrics['m3']*pm3)) * psc1\n",
    "tags_metrics['code_quality'] = ((tags_metrics['m1']*pm1) + (tags_metrics['m2']*pm2) + (tags_metrics['m3']*pm3)) * psc1\n",
    "profile_metrics['code_quality'] = ((profile_metrics['m1']*pm1) + (profile_metrics['m2']*pm2) + (profile_metrics['m3']*pm3)) * psc1\n",
    "processos_metrics['code_quality'] = ((processos_metrics['m1']*pm1) + (processos_metrics['m2']*pm2) + (processos_metrics['m3']*pm3)) * psc1\n",
    "\n",
    "frontEnd_metrics['testing_status'] = ((frontEnd_metrics['m4']*pm4) + (frontEnd_metrics['m5']*pm5) + (frontEnd_metrics['m6']*pm6)) * psc2\n",
    "tags_metrics['testing_status'] = ((tags_metrics['m4']*pm4) + (tags_metrics['m5']*pm5) + (tags_metrics['m6']*pm6)) * psc2\n",
    "profile_metrics['testing_status'] = ((profile_metrics['m4']*pm4) + (profile_metrics['m5']*pm5) + (profile_metrics['m6']*pm6)) * psc2\n",
    "processos_metrics['testing_status'] = ((processos_metrics['m4']*pm4) + (processos_metrics['m5']*pm5) + (processos_metrics['m6']*pm6)) * psc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "\n",
    "plt.plot(frontEnd_metrics['code_quality'], label='front', linewidth=3, marker='o', markersize=5)\n",
    "plt.plot(tags_metrics['code_quality'], label='tags', linewidth=3, marker='o', markersize=5)\n",
    "plt.plot(profile_metrics['code_quality'], label='profile', linewidth=3, marker='o', markersize=5)\n",
    "plt.plot(processos_metrics['code_quality'], label='registros', linewidth=3, marker='o', markersize=5)\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "\n",
    "plt.plot(frontEnd_metrics['testing_status'], label='front', linewidth=3, marker='o', markersize=5)\n",
    "plt.plot(tags_metrics['testing_status'], label='tags', linewidth=3, marker='o', markersize=5)\n",
    "plt.plot(profile_metrics['testing_status'], label='profile', linewidth=3, marker='o', markersize=5)\n",
    "plt.plot(processos_metrics['testing_status'], label='registros', linewidth=3, marker='o', markersize=5)\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.concat([\n",
    "frontEnd_metrics,\n",
    "tags_metrics,\n",
    "profile_metrics,\n",
    "processos_metrics\n",
    "], ignore_index=True)\n",
    "\n",
    "metrics_df['maintainability'] = metrics_df['code_quality'] * pc1\n",
    "metrics_df['Reliability'] = metrics_df['testing_status'] * pc2\n",
    "metrics_df['total'] = metrics_df['maintainability'] + metrics_df['Reliability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "\n",
    "plt.plot(metrics_df['maintainability'], label='maintainability', linewidth=3, marker='o', markersize=5)\n",
    "plt.plot(metrics_df['Reliability'], label='Reliability', linewidth=3, marker='o', markersize=5)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "#plt.ylim(.45,.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "\n",
    "plt.plot(metrics_df['total'], linewidth=3, marker='o', markersize=5)\n",
    "\n",
    "\n",
    "#plt.ylim(.45,.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS_DATE FORMAT: MM-DD-YYYY\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "now_str = now.strftime(\"%m-%d-%Y\")\n",
    "\n",
    "df = metrics_df[[\n",
    "    'm1',\n",
    "    'm2',\n",
    "    'm3',\n",
    "    'm4',\n",
    "    'm5',\n",
    "    'm6',\n",
    "    'maintainability',\n",
    "    'Reliability',\n",
    "    'total',\n",
    "    'ncloc',\n",
    "    'repository',\n",
    "    'version'\n",
    "]].copy()\n",
    "df['data'] = now_str\n",
    "\n",
    "#df.to_excel('data/fga-eps-mds-2021_1-Oraculo-DATASET-{}.xlsx'.format(now_str), index = False)\n",
    "#df.to_csv('data/fga-eps-mds-2021_1-Oraculo-DATASET-{}.csv'.format(now_str), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_or_list(x):\n",
    "    return x[0] if len(x) == 1 else x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_statistics(**kwargs):\n",
    "\n",
    "    data = {\n",
    "        'média': [kwargs[key].mean() for key in kwargs],\n",
    "        'moda': [element_or_list(kwargs[key].mode().values.tolist()) for key in kwargs],\n",
    "        'mediana': [kwargs[key].median() for key in kwargs],\n",
    "        'desvio padrão': [kwargs[key].std() for key in kwargs],\n",
    "        'variância': [kwargs[key].var() for key in kwargs],\n",
    "        'mínimo': [kwargs[key].min() for key in kwargs],\n",
    "        'máximo': [kwargs[key].max() for key in kwargs]\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame.from_dict(data, orient='index', columns=[key for key in kwargs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile(**kwargs):\n",
    "    \n",
    "    data = {\n",
    "        '5th' : [kwargs[key].quantile(0.05) for key in kwargs],\n",
    "        '10th': [kwargs[key].quantile(0.10) for key in kwargs],\n",
    "        '25th': [kwargs[key].quantile(0.25) for key in kwargs],\n",
    "        '50th': [kwargs[key].quantile(0.50) for key in kwargs],\n",
    "        '75th': [kwargs[key].quantile(0.75) for key in kwargs],\n",
    "        '90th': [kwargs[key].quantile(0.90) for key in kwargs],\n",
    "        '95th': [kwargs[key].quantile(0.95) for key in kwargs],\n",
    "        '99th': [kwargs[key].quantile(0.99) for key in kwargs],\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame.from_dict(data, orient='index', columns=[key for key in kwargs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(**kwargs):\n",
    "    \n",
    "    data = {k:v for k,v in kwargs.items()}\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    return df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot(maintainability, reliability):\n",
    "    data = [\n",
    "        maintainability,\n",
    "        reliability\n",
    "    ]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(data)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(frontEnd, profile, registros, tags):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    ax1 = frontEnd.plot(kind='scatter', x='code_quality', y='testing_status', color='r', label='front')\n",
    "    ax2 = profile.plot(kind='scatter', x='code_quality', y='testing_status', color='g', label='profile', ax=ax1)\n",
    "    ax3 = registros.plot(kind='scatter', x='code_quality', y='testing_status', color='b', label='registros', ax=ax1)\n",
    "    ax4 = tags.plot(kind='scatter', x='code_quality', y='testing_status', color='black', label='tags', ax=ax1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(total):\n",
    "    plt.hist(total, color='limegreen')\n",
    "    plt.axvline(total.mean(), linestyle='dashed', color='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### FrontEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = frontEnd_metrics.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Maintainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "descriptive_statistics(\n",
    "    complexity=df['m1'],\n",
    "    comments=df['m2'],\n",
    "    duplications=df['m3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "percentile(\n",
    "    complexity=df['m1'],\n",
    "    comments=df['m2'],\n",
    "    duplications=df['m3']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "descriptive_statistics(\n",
    "    passed_tests=df['m4'],\n",
    "    fast_tests=df['m5'],\n",
    "    test_coverage=df['m6']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "percentile(\n",
    "    passed_tests=df['m4'],\n",
    "    fast_tests=df['m5'],\n",
    "    test_coverage=df['m6']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correlation_matrix(\n",
    "    complexity=df['m1'],\n",
    "    comments=df['m2'],\n",
    "    duplications=df['m3'],\n",
    "    passed_tests=df['m4'],\n",
    "    fast_tests=df['m5'],\n",
    "    test_coverage=df['m6'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Box-Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "box_plot(df['code_quality'], df['testing_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = profile_metrics.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Maintainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "descriptive_statistics(\n",
    "    complexity=df['m1'],\n",
    "    comments=df['m2'],\n",
    "    duplications=df['m3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "percentile(\n",
    "    complexity=df['m1'],\n",
    "    comments=df['m2'],\n",
    "    duplications=df['m3']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "descriptive_statistics(\n",
    "    passed_tests=df['m4'],\n",
    "    fast_tests=df['m5'],\n",
    "    test_coverage=df['m6']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "percentile(\n",
    "    passed_tests=df['m4'],\n",
    "    fast_tests=df['m5'],\n",
    "    test_coverage=df['m6']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correlation_matrix(\n",
    "    complexity=df['m1'],\n",
    "    comments=df['m2'],\n",
    "    duplications=df['m3'],\n",
    "    passed_tests=df['m4'],\n",
    "    fast_tests=df['m5'],\n",
    "    test_coverage=df['m6'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Box-Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "box_plot(df['code_quality'], df['testing_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = processos_metrics.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Maintainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "descriptive_statistics(\n",
    "    complexity=df['m1'],\n",
    "    comments=df['m2'],\n",
    "    duplications=df['m3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "percentile(\n",
    "    complexity=df['m1'],\n",
    "    comments=df['m2'],\n",
    "    duplications=df['m3']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "descriptive_statistics(\n",
    "    passed_tests=df['m4'],\n",
    "    fast_tests=df['m5'],\n",
    "    test_coverage=df['m6']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "percentile(\n",
    "    passed_tests=df['m4'],\n",
    "    fast_tests=df['m5'],\n",
    "    test_coverage=df['m6']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correlation_matrix(\n",
    "    complexity=df['m1'],\n",
    "    comments=df['m2'],\n",
    "    duplications=df['m3'],\n",
    "    passed_tests=df['m4'],\n",
    "    fast_tests=df['m5'],\n",
    "    test_coverage=df['m6'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Box-Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "box_plot(df['code_quality'], df['testing_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tags_metrics.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Maintainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "descriptive_statistics(\n",
    "    complexity=df['m1'],\n",
    "    comments=df['m2'],\n",
    "    duplications=df['m3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "percentile(\n",
    "    complexity=df['m1'],\n",
    "    comments=df['m2'],\n",
    "    duplications=df['m3']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "descriptive_statistics(\n",
    "    passed_tests=df['m4'],\n",
    "    fast_tests=df['m5'],\n",
    "    test_coverage=df['m6']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "percentile(\n",
    "    passed_tests=df['m4'],\n",
    "    fast_tests=df['m5'],\n",
    "    test_coverage=df['m6']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correlation_matrix(\n",
    "    complexity=df['m1'],\n",
    "    comments=df['m2'],\n",
    "    duplications=df['m3'],\n",
    "    passed_tests=df['m4'],\n",
    "    fast_tests=df['m5'],\n",
    "    test_coverage=df['m6'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Box-Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "box_plot(df['code_quality'], df['testing_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(\n",
    "    frontEnd_metrics,\n",
    "    profile_metrics,\n",
    "    processos_metrics,\n",
    "    tags_metrics,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
